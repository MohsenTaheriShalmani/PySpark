{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Alternative approach\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4OJOaxO_6r-",
        "outputId": "94def0ce-e32e-4c24-e979-cb7117b8d15b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP9Oh1Jtoujz",
        "outputId": "3d5fcc00-8c2c-41d2-8bac-662d73b97890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import pyspark\n",
        "import json\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "WGs2gE8yAK1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract data by start and end\n",
        "def extraxtString(string,startString,endString):\n",
        "    startingPoint=0\n",
        "    lengthString=len(string)\n",
        "    myList = []\n",
        "    continueYes = 1\n",
        "    while(continueYes == 1 and startingPoint < lengthString):\n",
        "        temp1=string.find(startString,startingPoint)\n",
        "        if(temp1 != -1):\n",
        "            temp1 = temp1+len(startString)\n",
        "            temp2 = string.find(endString,temp1)\n",
        "            if(temp2 != -1):\n",
        "                #middleString=string[temp1:temp2]\n",
        "                middleString=string[(temp1-len(startString)):temp2]\n",
        "                myList.append(middleString)\n",
        "                #startingPoint=temp2+len(endString)\n",
        "                startingPoint=temp2\n",
        "            else:\n",
        "                continueYes=0\n",
        "        else:\n",
        "            continueYes=0\n",
        "    return myList"
      ],
      "metadata": {
        "id": "ur2DqTXj_l7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql.functions import from_json, udf, col, lower, regexp_replace, split, when\n",
        "from pyspark.sql.functions import rand #additional library for optimization\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, BooleanType\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import time\n"
      ],
      "metadata": {
        "id": "ql96Sct8GMyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install delta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za-SLtTto3mJ",
        "outputId": "a246ec8e-d59c-4796-f209-462bd0fe5e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting delta\n",
            "  Downloading delta-0.4.2.tar.gz (4.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: delta\n",
            "  Building wheel for delta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for delta: filename=delta-0.4.2-py3-none-any.whl size=2916 sha256=97e63f790cabf6d1c08acec003634fe787cc4e0d5de8cd60db79489c0e80a0e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/86/24/a486f14769cf86a2a9ce6b589a82b7414b14657c6fd515dc75\n",
            "Successfully built delta\n",
            "Installing collected packages: delta\n",
            "Successfully installed delta-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib-venn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li0YyDNIppq3",
        "outputId": "ea2bd4cf-f280-439e-da35-e76557431d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.10/dist-packages (0.11.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from matplotlib-venn) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from matplotlib-venn) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from matplotlib-venn) (1.11.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->matplotlib-venn) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################\n",
        "##############################################################"
      ],
      "metadata": {
        "id": "HMt21k7eq6je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql.functions import from_json, udf, col, lower, regexp_replace, split, when\n",
        "from pyspark.sql.functions import rand #additional library for optimization\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, BooleanType\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from delta import configure_spark_with_delta_pip"
      ],
      "metadata": {
        "id": "kZlXVBiGq9b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.base import Transformer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.types import IntegerType, StringType, ArrayType\n",
        "from pyspark.sql.functions import when"
      ],
      "metadata": {
        "id": "W7ijCOarzhJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Added last two .config to allocate memory and ensure 4 cores to be utilized\n",
        "#Set memory per exector to 2g to be able to handle the 5GB file\n",
        "builder = SparkSession.builder.appName(\"amazon\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .config(\"spark.executor.cores\", \"3\") \\\n",
        "    .config(\"spark.executor.instances\", \"4\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
      ],
      "metadata": {
        "id": "kgWMftDCpvoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dataframe from the txt file by reading it as json for formating\n",
        "df = spark.read.json(\"/content/drive/MyDrive/GoogleColab/Video_Games_5.json.gz\")"
      ],
      "metadata": {
        "id": "kPMX2u5jp3Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBbIC7AOqFuw",
        "outputId": "79015dad-4d6e-4981-b942-48b5dd4f1614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-------+--------------------+-----------+--------------+-----------------+-----+--------------------+--------------+--------+----+\n",
            "|      asin|image|overall|          reviewText| reviewTime|    reviewerID|     reviewerName|style|             summary|unixReviewTime|verified|vote|\n",
            "+----------+-----+-------+--------------------+-----------+--------------+-----------------+-----+--------------------+--------------+--------+----+\n",
            "|0700026657| NULL|    5.0|This game is a bi...|10 17, 2015|A1HP7NVNPFMA4N|      Ambrosia075| NULL|but when you do i...|    1445040000|    true|NULL|\n",
            "|0700026657| NULL|    4.0|I played it a whi...|07 27, 2015|A1JGAP0185YJI6|           travis| NULL|But in spite of t...|    1437955200|   false|NULL|\n",
            "|0700026657| NULL|    3.0|            ok game.|02 23, 2015|A1YJWEXHQBWK2B|Vincent G. Mezera| NULL|         Three Stars|    1424649600|    true|NULL|\n",
            "|0700026657| NULL|    2.0|found the game a ...|02 20, 2015|A2204E1TH211HT|       Grandma KR| NULL|           Two Stars|    1424390400|    true|NULL|\n",
            "|0700026657| NULL|    5.0|great game, I lov...|12 25, 2014|A2RF5B5H74JLPE|              jon| NULL|      love this game|    1419465600|    true|NULL|\n",
            "|0700026657| NULL|    4.0|i liked a lot som...|11 13, 2014|A11V6ZJ2FVQY1D|   IBRAHIM ALBADI| NULL|           Anno 2070|    1415836800|    true|NULL|\n",
            "|0700026657| NULL|    1.0|I'm an avid gamer...| 08 2, 2014|A1KXJ1ELZIU05C|       Creation27| NULL|Avoid This Game -...|    1406937600|   false|NULL|\n",
            "|0700026657| NULL|    5.0|I bought this gam...| 03 3, 2014|A1WK5I4874S3O2|       WhiteSkull| NULL|A very good game ...|    1393804800|    true|NULL|\n",
            "|0700026657| NULL|    5.0|I have played the...|02 21, 2014| AV969NA4CBP10|  Travis B. Moore| NULL|Anno 2070 more li...|    1392940800|    true|NULL|\n",
            "|0700026657| NULL|    4.0|I liked it and ha...|06 27, 2013|A1EO9BFUHTGWKZ|         johnnyz3| NULL|          Pretty fun|    1372291200|    true|NULL|\n",
            "|0700026657| NULL|    4.0|4 Stars because t...|12 28, 2012|A2M8JTIST6FPZZ|  Amazon Customer| NULL|My boys enjoys th...|    1356652800|    true|NULL|\n",
            "|0700026657| NULL|    1.0|I've bought and p...|05 15, 2012|A1LMJ9W8UX1H5B|           Rob NY| NULL|     SAY NO TO DRM!!|    1337040000|   false|  28|\n",
            "|0700099867| NULL|    5.0|Loved playing Dir...|08 14, 2011| AN3YYDZAS3O1Y|              Bob| NULL|A step up from Di...|    1313280000|    true|  11|\n",
            "|0700099867| NULL|    5.0|I'm not quite fin...|06 28, 2011|A38NXTZUFB1O2K|             FiSH| NULL| Best in the series!|    1309219200|   false|NULL|\n",
            "|0700099867| NULL|    5.0|lot of people don...|06 18, 2011|A15PIAQT55GNCA|        Suk W. Yu| NULL|this games is ama...|    1308355200|   false|NULL|\n",
            "|0700099867| NULL|    4.0|I had Dirt 2 on X...|06 14, 2011|A361M14PU2GUEG|       Angry Ryan| NULL|              DIRT 3|    1308009600|    true|   2|\n",
            "|0700099867| NULL|    5.0|This is a must ha...|06 13, 2011|A2LQCBLLJVVR5T|         Timmiley| NULL|BEST GRAPHICS OF ...|    1307923200|   false|  14|\n",
            "|0700026398| NULL|    1.0|I'm sure I would ...|05 18, 2013|A1NQ759X8WPIVV|             Lynn| NULL|      Requires steam|    1368835200|    true|NULL|\n",
            "|0700026398| NULL|    1.0|Update June 2013:...|01 10, 2013| APFCXOFX0KUPN|               --| NULL|               Skip!|    1357776000|   false|NULL|\n",
            "|0700026398| NULL|    4.0|I will open with ...|12 26, 2012|A2GPRA9HHLOC4B|      Wicasawakan| NULL|Great game with d...|    1356480000|   false|  12|\n",
            "+----------+-----+-------+--------------------+-----------+--------------+-----------------+-----+--------------------+--------------+--------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.select(\"overall\",\"reviewText\")\n",
        "#df.show(truncate=False)"
      ],
      "metadata": {
        "id": "FvIHmkzEsFzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a delta frame from the df and save to the hdfs\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/drive/MyDrive/GoogleColab\")"
      ],
      "metadata": {
        "id": "BE3hrPfssWqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kAaJVofDs1oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fetch the delta table from hdfs\n",
        "delta_df = spark.read.format(\"delta\").load(\"/content/drive/MyDrive/GoogleColab\")\n",
        "delta_df.show(3,truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHr8a5tlqtpp",
        "outputId": "9423a7a2-c7c2-46dd-fa1f-3f47624b9b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|overall|reviewText                                                                                                                                                                                                                                                                                               |\n",
            "+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|5.0    |This game is a bit hard to get the hang of, but when you do it's great.                                                                                                                                                                                                                                  |\n",
            "|4.0    |I played it a while but it was alright. The steam was a bit of trouble. The more they move these game to steam the more of a hard time I have activating and playing a game. But in spite of that it was fun, I liked it. Now I am looking forward to anno 2205 I really want to play my way to the moon.|\n",
            "|3.0    |ok game.                                                                                                                                                                                                                                                                                                 |\n",
            "+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'overall' to binary (0 or 1)\n",
        "delta_df = delta_df.withColumn(\"overall\", when(delta_df[\"overall\"] < 3, 0).otherwise(1))\n",
        "#delta_df.show(3,truncate=False)"
      ],
      "metadata": {
        "id": "SFcJlHVytW5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom transformer to remove special characters\n",
        "class RemoveSpecialCharactersTransformer(Transformer):\n",
        "    def __init__(self, inputCol, outputCol):\n",
        "        super(RemoveSpecialCharactersTransformer, self).__init__()\n",
        "        self.inputCol = inputCol\n",
        "        self.outputCol = outputCol\n",
        "\n",
        "    def _transform(self, df: DataFrame) -> DataFrame:\n",
        "#        print(f\"Ran _transform with data {df[self.inputCol]}\")\n",
        "        return df.withColumn(self.outputCol, udf(remove_special_characters)(df[self.inputCol]))\n",
        "\n",
        "# Define the UDF to remove special characters\n",
        "def remove_special_characters(text):\n",
        "    print(text, '\\n')\n",
        "    if type(text)==str:\n",
        "        return re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "# Create instances of the custom transformers\n",
        "remove_special_chars = RemoveSpecialCharactersTransformer(inputCol=\"reviewText\", outputCol=\"cleanText\")\n"
      ],
      "metadata": {
        "id": "X9oNed9mtejk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the reviewText\n",
        "tokenizer = Tokenizer(inputCol=\"cleanText\", outputCol=\"token\")"
      ],
      "metadata": {
        "id": "WO5LP2k-t8Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the StopWordsRemover\n",
        "stop_words_remover = StopWordsRemover(inputCol=\"token\", outputCol=\"words\")\n",
        "\n",
        "# Vectorize the filtered words using HashingTF\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")"
      ],
      "metadata": {
        "id": "NQ7ymJFHtieb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "train_data, test_data = delta_df.randomSplit([0.7, 0.3])"
      ],
      "metadata": {
        "id": "EfLN0rPsuZuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Naive Bayes model\n",
        "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"overall\")\n",
        "\n",
        "# Create a pipeline with Tokenizer, StopWordsRemover, HashingTF, and Naive Bayes\n",
        "pipeline = Pipeline(stages=[remove_special_chars,tokenizer, stop_words_remover, hashingTF, nb])\n"
      ],
      "metadata": {
        "id": "ke3rdpzYo1Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = pipeline.fit(train_data)"
      ],
      "metadata": {
        "id": "VOQmI9ckvZpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate the model using MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"overall\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v-f0_R4x7Xn",
        "outputId": "1550bbd9-b27f-4c2b-9e05-81f9ba2678f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9117300008045268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions on new reviewText\n",
        "new_review_text = \"good negative bad ouch terrible\"\n",
        "new_data = spark.createDataFrame([(new_review_text,)], [\"reviewText\"])\n",
        "new_predictions = model.transform(new_data)\n",
        "\n",
        "# Show the prediction for the new reviewText\n",
        "new_predictions.select(\"reviewText\", \"prediction\").show(truncate=False)\n",
        "\n",
        "#repartitioning technique\n",
        "#current_partitions= df.rdd.getNumPartitions()\n",
        "#print(f\"Current number of partitions: {current_partitions}\")\n",
        "\"\"\"desired_partition=4\n",
        "#df_repartitioned=df.repartition(desired_partition)\n",
        "#new_partitions=df_repartitioned.rdd.getNumPartitions()\n",
        "#print(f\"New number of partitions: {new_partitions}\")\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "5YyBiEKvzWV7",
        "outputId": "7e0238bd-21cb-46f3-cd23-d0285dec5a34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-7f033ec6e2ea>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# make predictions on new reviewText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnew_review_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"good negative bad ouch terrible\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_review_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"reviewText\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnew_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             )\n\u001b[0;32m-> 1443\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# convert python objects to sql data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0minternal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtupled_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minternal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \"\"\"\n\u001b[0;32m--> 783\u001b[0;31m         \u001b[0mnumSlices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumSlices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnumSlices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mdefaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#calculating the end time and duration of executing\n",
        "\"\"\"end_time=time.time()\n",
        "execution_time=end_time- start_time\n",
        "print(f\"Execution time: {execution_time} seconds\")\n",
        "\"\"\"\n",
        "#salting\n",
        "\"\"\"num_salt_buckets=40\n",
        "df_salted=df.withColumn(\"salt\", (rand()*num_salt_buckets).cast(\"int\"))\n",
        "df_salted_repartitionized=df_salted.repartition(\"salt\")\"\"\"\n",
        "\n",
        "# Unpersist the DataFrame, free up the memory\n",
        "#df.unpersist()\n",
        "\n",
        "# Stop the Spark session\n",
        "#spark.stop()"
      ],
      "metadata": {
        "id": "MqhEwcCFyAKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k00VcRVpyAUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spark = SparkSession.builder.master('local[*]').config(\"spark.driver.memory\", \"30g\").appName('myProject_data_intensive').getOrCreate()\n"
      ],
      "metadata": {
        "id": "6PTJqhwnISG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#Create empty pyspark dataframe\n",
        "schema = StructType([StructField('overall', StringType(), True),\n",
        "                     StructField('reviewText', StringType(), True)])\n",
        "emptyRDD = spark.sparkContext.emptyRDD()\n",
        "df = spark.createDataFrame(emptyRDD,schema)\n",
        "#df.printSchema()\n",
        "df.show()\n",
        "\n",
        "\n",
        "dataPath=\"/content/drive/MyDrive/Colab Notebooks (1)/textData\"\n",
        "\n",
        "columns = [\"overall\", \"reviewText\"]\n",
        "start1=\"overall: \"\n",
        "end1=\".0,\"\n",
        "start2=\"reviewText: \"\n",
        "end2=\", summary\"\n",
        "\n",
        "for filename in os.listdir(dataPath):\n",
        "    print(filename)\n",
        "    mystring = open(os.path.join(dataPath, filename)).read().replace('\\n', '')\n",
        "    ListReviewers = extraxtString(mystring,\"overall:\",\"overall:\")\n",
        "\n",
        "    myList=[]\n",
        "    for i in range(0,len(ListReviewers)):\n",
        "        s=ListReviewers[i]\n",
        "        if \"overall\" and \"reviewText\" in s:\n",
        "            string1 = s[s.find(start1)+len(start1):s.rfind(end1)]\n",
        "            string2 = s[s.find(start2)+len(start2):s.rfind(end2)]\n",
        "            myList.append([string1,string2])\n",
        "\n",
        "    print(filename,\"is structured.\")\n",
        "    #dataTemp = myList[0:1000]\n",
        "    dataTemp = myList\n",
        "    dataFrameTemp = spark.createDataFrame(dataTemp, columns)\n",
        "    #dataFrameTemp.show()\n",
        "\n",
        "    # concatinate data frames\n",
        "    df = df.union(dataFrameTemp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "t_tf8pT5GAGm",
        "outputId": "fdba0103-a3f0-489b-dba3-fddd04ed4257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|overall|reviewText|\n",
            "+-------+----------+\n",
            "+-------+----------+\n",
            "\n",
            "Video_Games_5_test.txt\n",
            "Video_Games_5_test.txt is structured.\n",
            "Arts_Crafts_and_Sewing_5_test.txt\n",
            "Arts_Crafts_and_Sewing_5_test.txt is structured.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9a8485f49ce2>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#dataTemp = myList[0:1000]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mdataTemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdataFrameTemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataTemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;31m#dataFrameTemp.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             )\n\u001b[0;32m-> 1443\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# convert python objects to sql data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0minternal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoInternal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtupled_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minternal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonParallelizeServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serialize_to_jvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreateRDDServer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_serialize_to_jvm\u001b[0;34m(self, data, serializer, reader_func, server_func)\u001b[0m\n\u001b[1;32m    868\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                     \u001b[0mtempFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreader_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtempFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;31m# we eagerly reads the file so we can delete right after.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mreader_func\u001b[0;34m(temp_filename)\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mreader_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_filename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJavaObject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadRDDFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcreateRDDServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJavaObject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile.\n: java.lang.OutOfMemoryError: Java heap space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfWdBoKxKnH4",
        "outputId": "bb8f1436-5c6e-48d9-e5de-8678b756aef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+\n",
            "|overall|          reviewText|\n",
            "+-------+--------------------+\n",
            "|      4|I've been using D...|\n",
            "|      4|The demo is done ...|\n",
            "|      5|If you've been wa...|\n",
            "|      5|I've been creatin...|\n",
            "|      5|I decided (after ...|\n",
            "|      5|The video is well...|\n",
            "|      5|I spent several h...|\n",
            "|      5|I have had Dreamw...|\n",
            "|      5|I have also taken...|\n",
            "|      5|Even though I use...|\n",
            "|      3|I waited to compl...|\n",
            "|      5|As someone who ha...|\n",
            "|      5|Adobe approved \\A...|\n",
            "|      5|This is a greate ...|\n",
            "|      3|There are over 10...|\n",
            "|      5|I am not an avid ...|\n",
            "|      5|I am a long time ...|\n",
            "|      4|The \\Learn by Vid...|\n",
            "|      4|The presenter her...|\n",
            "|      4|This certified as...|\n",
            "+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df.toPandas().to_csv('/content/drive/MyDrive/Colab Notebooks (1)/mycsv.csv')"
      ],
      "metadata": {
        "id": "abIPItENHvDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xe_XYHizL47j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}